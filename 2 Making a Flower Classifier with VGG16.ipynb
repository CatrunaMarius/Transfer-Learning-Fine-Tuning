{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bit43b0aca25f3a478a89236e706d769e39",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facem o clasificarea a florilor cu VGG16\n",
    "## Incarcam modelul VGG16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "WARNING:tensorflow:From C:\\Users\\Harum\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n"
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "#VGG16 a fost proiectat sa functioneze cu imagini cu dimensiunea de 224x224 pixel\n",
    "img_rows = 224\n",
    "img_cols = 224\n",
    "\n",
    "#Incarcare model VGG16\n",
    "vgg16 = VGG16(weights = \"imagenet\",\n",
    "              include_top = False,\n",
    "              input_shape = (img_rows, img_cols, 3)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 InputLayer False\n1 Conv2D True\n2 Conv2D True\n3 MaxPooling2D True\n4 Conv2D True\n5 Conv2D True\n6 MaxPooling2D True\n7 Conv2D True\n8 Conv2D True\n9 Conv2D True\n10 MaxPooling2D True\n11 Conv2D True\n12 Conv2D True\n13 Conv2D True\n14 MaxPooling2D True\n15 Conv2D True\n16 Conv2D True\n17 Conv2D True\n18 MaxPooling2D True\n"
    }
   ],
   "source": [
    "#printeaza layers\n",
    "for (i, layer) in enumerate(vgg16.layers):\n",
    "    print(str(i)+ \" \"+layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incheata toate layers cu exceptia primelor 4 de sus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 InputLayer False\n1 Conv2D False\n2 Conv2D False\n3 MaxPooling2D False\n4 Conv2D False\n5 Conv2D False\n6 MaxPooling2D False\n7 Conv2D False\n8 Conv2D False\n9 Conv2D False\n10 MaxPooling2D False\n11 Conv2D False\n12 Conv2D False\n13 Conv2D False\n14 MaxPooling2D False\n15 Conv2D False\n16 Conv2D False\n17 Conv2D False\n18 MaxPooling2D False\n"
    }
   ],
   "source": [
    "#VGG16 a fost proiectat sa functioneze cu imagini cu dimensiunea de 224x224 pixel\n",
    "img_rows = 224\n",
    "img_cols = 224\n",
    "\n",
    "#Reîncărcați modelul VGG16 fără straturile superioare sau FC\n",
    "vgg16 =VGG16(weights = 'imagenet',\n",
    "             include_top = False,\n",
    "             input_shape = (img_rows, img_cols, 3)\n",
    ")\n",
    "\n",
    "#Aici vom incheta ultimele 4 straturi/layers\n",
    "#În mod implicit, straturile sunt instruibile ca True\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#printeaza layers\n",
    "for (i, layer) in enumerate(vgg16.layers):\n",
    "    print(str(i)+ \" \"+layer.__class__.__name__, layer.trainable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cream o functie care returneaza capul FC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTopModel(botton_model, num_classes, D=256):\n",
    "    \"\"\"creează partea superioară sau capul modelului care va fi plasat pe partea de sus a straturilor de jos\"\"\"\n",
    "    top_model = botton_model.output\n",
    "    top_model = Flatten(name= \"flatten\")(top_model)\n",
    "    top_model = Dense(D, activation = \"relu\")(top_model)\n",
    "    top_model = Dropout(0.3)(top_model)\n",
    "    top_model = Dense(num_classes, activation= \"softmax\")(top_model)\n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adauga capul FC inapoi in VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         (None, 224, 224, 3)       0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 256)               6422784   \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 17)                4369      \n=================================================================\nTotal params: 21,141,841\nTrainable params: 6,427,153\nNon-trainable params: 14,714,688\n_________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "num_classes = 17\n",
    "FC_Head = addTopModel(vgg16, num_classes)\n",
    "model =Model(inputs=vgg16.input, outputs=FC_Head)\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# incarca baza de date cu florii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Found 1190 images belonging to 17 classes.\nFound 170 images belonging to 17 classes.\n"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir = 'c:/Users/Harum/Documents/15/17_flowers/train'\n",
    "validation_data_dir = 'c:/Users/Harum/Documents/15/17_flowers/validation'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "                   rescale=1./255,\n",
    "                   rotation_range=20,\n",
    "                   width_shift_range=0.2,\n",
    "                   height_shift_range=0.2,\n",
    "                   horizontal_flip=True,\n",
    "                   fill_mode='nearest'\n",
    ")\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#Schimba batchsize in concordanta cu memoria RAM in sistem\n",
    "train_batchsize = 16\n",
    "val_batchsize = 10\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                   train_data_dir,\n",
    "                   target_size=(img_rows, img_cols),\n",
    "                   batch_size=train_batchsize,\n",
    "                   class_mode='categorical'\n",
    ")\n",
    "validations_generator = validation_datagen.flow_from_directory(\n",
    "                         validation_data_dir,\n",
    "                         target_size=(img_rows, img_cols),\n",
    "                         batch_size=val_batchsize,\n",
    "                         class_mode= 'categorical',\n",
    "                         shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruire stratul/layers de sus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/10\n18/18 [==============================] - 5s 263ms/step - loss: 1.2212 - accuracy: 0.6978 - val_loss: 0.6690 - val_accuracy: 0.7500\n\nEpoch 00001: val_loss improved from inf to 0.66903, saving model to c:/Users/Harum/Documents/15/flowers_vgg.h5\nEpoch 2/10\n18/18 [==============================] - 4s 227ms/step - loss: 0.9020 - accuracy: 0.6910 - val_loss: 1.4032 - val_accuracy: 0.7500\n\nEpoch 00002: val_loss did not improve from 0.66903\nEpoch 3/10\n18/18 [==============================] - 4s 242ms/step - loss: 0.8170 - accuracy: 0.7812 - val_loss: 0.2575 - val_accuracy: 1.0000\n\nEpoch 00003: val_loss improved from 0.66903 to 0.25750, saving model to c:/Users/Harum/Documents/15/flowers_vgg.h5\nEpoch 4/10\n18/18 [==============================] - 4s 239ms/step - loss: 0.8256 - accuracy: 0.7292 - val_loss: 0.0566 - val_accuracy: 1.0000\n\nEpoch 00004: val_loss improved from 0.25750 to 0.05663, saving model to c:/Users/Harum/Documents/15/flowers_vgg.h5\nEpoch 5/10\n18/18 [==============================] - 5s 254ms/step - loss: 0.7562 - accuracy: 0.7396 - val_loss: 0.0254 - val_accuracy: 1.0000\n\nEpoch 00005: val_loss improved from 0.05663 to 0.02539, saving model to c:/Users/Harum/Documents/15/flowers_vgg.h5\nEpoch 6/10\n18/18 [==============================] - 4s 236ms/step - loss: 0.9344 - accuracy: 0.6727 - val_loss: 0.0012 - val_accuracy: 0.7000\n\nEpoch 00006: val_loss improved from 0.02539 to 0.00118, saving model to c:/Users/Harum/Documents/15/flowers_vgg.h5\nEpoch 7/10\n18/18 [==============================] - 4s 240ms/step - loss: 0.7541 - accuracy: 0.7708 - val_loss: 1.6376e-04 - val_accuracy: 0.9500\n\nEpoch 00007: val_loss improved from 0.00118 to 0.00016, saving model to c:/Users/Harum/Documents/15/flowers_vgg.h5\nEpoch 8/10\n18/18 [==============================] - 4s 246ms/step - loss: 0.8197 - accuracy: 0.7396 - val_loss: 1.1045 - val_accuracy: 0.8000\n\nEpoch 00008: val_loss did not improve from 0.00016\nEpoch 9/10\n18/18 [==============================] - 4s 242ms/step - loss: 0.6967 - accuracy: 0.7770 - val_loss: 1.6503 - val_accuracy: 0.7500\n\nEpoch 00009: val_loss did not improve from 0.00016\nEpoch 10/10\n18/18 [==============================] - 4s 249ms/step - loss: 0.7571 - accuracy: 0.7431 - val_loss: 0.0360 - val_accuracy: 0.9500\nRestoring model weights from the end of the best epoch\n\nEpoch 00010: val_loss did not improve from 0.00016\nEpoch 00010: early stopping\n"
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"c:/Users/Harum/Documents/15/flowers_vgg.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only=True,\n",
    "                             verbose=1\n",
    ")\n",
    "\n",
    "earlystop = EarlyStopping(monitor= 'val_loss',\n",
    "                         min_delta=0,\n",
    "                         patience=3,\n",
    "                         verbose=1,\n",
    "                         restore_best_weights=True\n",
    "                         )\n",
    "\n",
    "#pune  call backs intrun callback list\n",
    "callbacks = [earlystop, checkpoint]\n",
    "\n",
    "#Utilizam o rata de invatare doarte mica\n",
    "model.compile(RMSprop(0.001),'categorical_crossentropy', ['accuracy'])\n",
    "\n",
    "#Introduce numarul de instruire si validare \n",
    "nb_train_samples = 1190\n",
    "nb_validation_samples = 170\n",
    "#parametri de instruire\n",
    "epochs = 10\n",
    "batch_size =64\n",
    "\n",
    "# instruire model\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs= epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=validations_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putem mari viteza modelului\n",
    " sa incercam sa scadem dimensiunea imagini la 64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 InputLayer False\n1 Conv2D False\n2 Conv2D False\n3 MaxPooling2D False\n4 Conv2D False\n5 Conv2D False\n6 MaxPooling2D False\n7 Conv2D False\n8 Conv2D False\n9 Conv2D False\n10 MaxPooling2D False\n11 Conv2D False\n12 Conv2D False\n13 Conv2D False\n14 MaxPooling2D False\n15 Conv2D False\n16 Conv2D False\n17 Conv2D False\n18 MaxPooling2D False\n"
    }
   ],
   "source": [
    "#Setam dimensiunea inputului la 64 x 64 pixeli\n",
    "img_rows = 64\n",
    "img_cols = 64\n",
    "\n",
    "#Reîncărcați modelul VGG16 fără straturile superioare sau FC\n",
    "vgg16 = VGG16(weights = 'imagenet',\n",
    "            include_top = False,\n",
    "            input_shape = (img_rows, img_cols, 3)\n",
    " )\n",
    "\n",
    "#Aici vom incheta ultimele 4 straturi/layers\n",
    "#În mod implicit, straturile sunt instruibile ca True\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#printeaza layers\n",
    "for (i, layer) in enumerate(vgg16.layers):\n",
    "    print(str(i)+ \" \"+layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cream un noe model folosind o imagine a dimensiuni de 64 x 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Found 1190 images belonging to 17 classes.\nFound 170 images belonging to 17 classes.\nModel: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_6 (InputLayer)         (None, 64, 64, 3)         0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 2048)              0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 256)               524544    \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 17)                4369      \n=================================================================\nTotal params: 15,243,601\nTrainable params: 528,913\nNon-trainable params: 14,714,688\n_________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir = 'c:/Users/Harum/Documents/15/17_flowers/train'\n",
    "validation_data_dir = 'c:/Users/Harum/Documents/15/17_flowers/validation'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "                   rescale=1./255,\n",
    "                   rotation_range=20,\n",
    "                   width_shift_range=0.2,\n",
    "                   height_shift_range=0.2,\n",
    "                   horizontal_flip=True,\n",
    "                   fill_mode='nearest'\n",
    ")\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#Schimba batchsize in concordanta cu memoria RAM in sistem\n",
    "train_batchsize = 16\n",
    "val_batchsize = 10\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                   train_data_dir,\n",
    "                   target_size=(img_rows, img_cols),\n",
    "                   batch_size=train_batchsize,\n",
    "                   class_mode='categorical'\n",
    ")\n",
    "validations_generator = validation_datagen.flow_from_directory(\n",
    "                         validation_data_dir,\n",
    "                         target_size=(img_rows, img_cols),\n",
    "                         batch_size=val_batchsize,\n",
    "                         class_mode= 'categorical',\n",
    "                         shuffle=False)\n",
    "\n",
    "#Reîncărcați modelul VGG16 fără straturile superioare sau FC\n",
    "vgg16 = VGG16(weights = 'imagenet',\n",
    "            include_top = False,\n",
    "            input_shape = (img_rows, img_cols, 3)\n",
    " )\n",
    "\n",
    "#Aici vom incheta ultimele 4 straturi/layers\n",
    "#În mod implicit, straturile sunt instruibile ca True\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#numarul de clase in baza de date Flowers-17\n",
    "num_classes =17\n",
    "\n",
    "FC_Head = addTopModel(vgg16, num_classes)\n",
    "\n",
    "model = Model(inputs=vgg16.input, outputs=FC_Head)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instruire folosind imagini cu dimensiunea de 64 x 64 este mult mai rapid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/25\n74/74 [==============================] - 5s 70ms/step - loss: 1.9703 - accuracy: 0.4412 - val_loss: 1.7163 - val_accuracy: 0.5600\n\nEpoch 00001: val_loss improved from inf to 1.71630, saving model to c:/Users/Harum/Documents/15/flowers_vgg_64.h5\nEpoch 2/25\n74/74 [==============================] - 5s 71ms/step - loss: 1.8240 - accuracy: 0.4676 - val_loss: 1.2416 - val_accuracy: 0.5200\n\nEpoch 00002: val_loss improved from 1.71630 to 1.24156, saving model to c:/Users/Harum/Documents/15/flowers_vgg_64.h5\nEpoch 3/25\n74/74 [==============================] - 5s 71ms/step - loss: 1.7119 - accuracy: 0.5119 - val_loss: 1.3039 - val_accuracy: 0.6200\n\nEpoch 00003: val_loss did not improve from 1.24156\nEpoch 4/25\n74/74 [==============================] - 5s 71ms/step - loss: 1.5778 - accuracy: 0.5358 - val_loss: 1.5709 - val_accuracy: 0.5200\n\nEpoch 00004: val_loss did not improve from 1.24156\nEpoch 5/25\n74/74 [==============================] - 5s 71ms/step - loss: 1.5599 - accuracy: 0.5298 - val_loss: 2.0808 - val_accuracy: 0.6600\n\nEpoch 00005: val_loss did not improve from 1.24156\n\nEpoch 00005: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\nEpoch 6/25\n74/74 [==============================] - 5s 71ms/step - loss: 1.4692 - accuracy: 0.5767 - val_loss: 0.8749 - val_accuracy: 0.6600\n\nEpoch 00006: val_loss improved from 1.24156 to 0.87487, saving model to c:/Users/Harum/Documents/15/flowers_vgg_64.h5\nEpoch 7/25\n74/74 [==============================] - 5s 70ms/step - loss: 1.4400 - accuracy: 0.5769 - val_loss: 0.7088 - val_accuracy: 0.6100\n\nEpoch 00007: val_loss improved from 0.87487 to 0.70884, saving model to c:/Users/Harum/Documents/15/flowers_vgg_64.h5\nEpoch 8/25\n74/74 [==============================] - 5s 68ms/step - loss: 1.4269 - accuracy: 0.5859 - val_loss: 1.1416 - val_accuracy: 0.6800\n\nEpoch 00008: val_loss did not improve from 0.70884\nEpoch 9/25\n74/74 [==============================] - 5s 70ms/step - loss: 1.4044 - accuracy: 0.5869 - val_loss: 1.6898 - val_accuracy: 0.5000\n\nEpoch 00009: val_loss did not improve from 0.70884\nEpoch 10/25\n74/74 [==============================] - 5s 69ms/step - loss: 1.4175 - accuracy: 0.5724 - val_loss: 1.8043 - val_accuracy: 0.7400\n\nEpoch 00010: val_loss did not improve from 0.70884\n\nEpoch 00010: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\nEpoch 11/25\n74/74 [==============================] - 5s 70ms/step - loss: 1.3609 - accuracy: 0.5988 - val_loss: 0.6990 - val_accuracy: 0.5900\n\nEpoch 00011: val_loss improved from 0.70884 to 0.69900, saving model to c:/Users/Harum/Documents/15/flowers_vgg_64.h5\nEpoch 12/25\n74/74 [==============================] - 5s 69ms/step - loss: 1.3555 - accuracy: 0.6134 - val_loss: 1.9638 - val_accuracy: 0.6200\n\nEpoch 00012: val_loss did not improve from 0.69900\nEpoch 13/25\n74/74 [==============================] - 5s 70ms/step - loss: 1.3484 - accuracy: 0.6107 - val_loss: 2.0523 - val_accuracy: 0.7000\n\nEpoch 00013: val_loss did not improve from 0.69900\nEpoch 14/25\n74/74 [==============================] - 5s 71ms/step - loss: 1.4021 - accuracy: 0.5767 - val_loss: 2.6055 - val_accuracy: 0.5700\n\nEpoch 00014: val_loss did not improve from 0.69900\n\nEpoch 00014: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\nEpoch 15/25\n74/74 [==============================] - 5s 67ms/step - loss: 1.4004 - accuracy: 0.5802 - val_loss: 0.5561 - val_accuracy: 0.7400\n\nEpoch 00015: val_loss improved from 0.69900 to 0.55607, saving model to c:/Users/Harum/Documents/15/flowers_vgg_64.h5\nEpoch 16/25\n74/74 [==============================] - 5s 70ms/step - loss: 1.3549 - accuracy: 0.5945 - val_loss: 0.3026 - val_accuracy: 0.5500\n\nEpoch 00016: val_loss improved from 0.55607 to 0.30265, saving model to c:/Users/Harum/Documents/15/flowers_vgg_64.h5\nEpoch 17/25\n74/74 [==============================] - 5s 67ms/step - loss: 1.3590 - accuracy: 0.5860 - val_loss: 0.6702 - val_accuracy: 0.6700\n\nEpoch 00017: val_loss did not improve from 0.30265\nEpoch 18/25\n74/74 [==============================] - 5s 71ms/step - loss: 1.3632 - accuracy: 0.6031 - val_loss: 0.7381 - val_accuracy: 0.6800\n\nEpoch 00018: val_loss did not improve from 0.30265\nEpoch 19/25\n74/74 [==============================] - 5s 71ms/step - loss: 1.3925 - accuracy: 0.5938 - val_loss: 0.8780 - val_accuracy: 0.5800\n\nEpoch 00019: val_loss did not improve from 0.30265\n\nEpoch 00019: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\nEpoch 20/25\n74/74 [==============================] - 5s 68ms/step - loss: 1.3674 - accuracy: 0.5869 - val_loss: 1.0396 - val_accuracy: 0.6700\n\nEpoch 00020: val_loss did not improve from 0.30265\nEpoch 21/25\n74/74 [==============================] - 5s 70ms/step - loss: 1.3775 - accuracy: 0.5980 - val_loss: 1.5540 - val_accuracy: 0.5400\nRestoring model weights from the end of the best epoch\n\nEpoch 00021: val_loss did not improve from 0.30265\nEpoch 00021: early stopping\n"
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"c:/Users/Harum/Documents/15/flowers_vgg_64.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only=True,\n",
    "                             verbose=1\n",
    ")\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                        min_delta=0,\n",
    "                        patience=5,\n",
    "                        verbose=1,\n",
    "                        restore_best_weights=True\n",
    "                        )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor= 'val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              restore_best_weights = True \n",
    "                                  )\n",
    "\n",
    "#punem call backs intro lista callbacks\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "#Folosim o rata de invatare foarte mica\n",
    "model.compile(RMSprop(0.0001), 'categorical_crossentropy', ['accuracy'])\n",
    "\n",
    "#Introduce numarul de instruire si validare \n",
    "nb_train_samples = 1190\n",
    "nb_validation_samples = 170\n",
    "\n",
    "#parametri de instruire\n",
    "epochs=25\n",
    "batch_size = 16\n",
    "\n",
    "# instruire\n",
    "history= model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch= nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=validations_generator,\n",
    "    validation_steps= nb_validation_samples //batch_size\n",
    ")\n"
   ]
  }
 ]
}